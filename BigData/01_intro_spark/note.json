{"paragraphs":[{"text":"print(s\"\"\"%html\n<center>\n    <h1><a href=\"http://diplodatos.famaf.unc.edu.ar/\">Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones</a></h1>\n    <h2>Curso <a href=\"https://sites.google.com/view/eleccion-optativas-diplodatos/programaci%C3%B3n-distribu%C3%ADda-sobre-grandes-vol%C3%BAmenes-de-datos\">Programación Distribuida sobre Grandes Volúmenes de Datos</a></h2>\n</center>\n\n<br>\n\n<h3 style=\"text-align:center;\"> Damián Barsotti  </h3>\n\n<h3 style=\"text-align:center;\">\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    Facultad de Matemática Astronomía Física y Computación\n    </a>\n<br/>\n    <a href=\"http://www.unc.edu.ar\">\n    Universidad Nacional de Córdoba\n    </a>\n<br/>\n    <center>\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    <img src=\"$baseDir/comun/logo%20UNC%20FAMAF%202016.png\" alt=\"Drawing\" style=\"width:50%;\"/>\n    </a>\n    </center>\n</h3>\n\n<p style=\"font-size:15px;\">\n    <br />\n        This work is licensed under a\n        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">\n        <img alt=\"Creative Commons License\" style=\"border-width:0;vertical-align:middle;float:right\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" />\n    </a>\n</p>\n\"\"\")\n","user":"anonymous","dateUpdated":"2019-12-12T01:02:41-0300","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"fontSize":9,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<center>\n    <h1><a href=\"http://diplodatos.famaf.unc.edu.ar/\">Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones</a></h1>\n    <h2>Curso <a href=\"https://sites.google.com/view/eleccion-optativas-diplodatos/programaci%C3%B3n-distribu%C3%ADda-sobre-grandes-vol%C3%BAmenes-de-datos\">Programación Distribuida sobre Grandes Volúmenes de Datos</a></h2>\n</center>\n\n<br>\n\n<h3 style=\"text-align:center;\"> Damián Barsotti  </h3>\n\n<h3 style=\"text-align:center;\">\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    Facultad de Matemática Astronomía Física y Computación\n    </a>\n<br/>\n    <a href=\"http://www.unc.edu.ar\">\n    Universidad Nacional de Córdoba\n    </a>\n<br/>\n    <center>\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    <img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/comun/logo%20UNC%20FAMAF%202016.png\" alt=\"Drawing\" style=\"width:50%;\"/>\n    </a>\n    </center>\n</h3>\n\n<p style=\"font-size:15px;\">\n    <br />\n        This work is licensed under a\n        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">\n        <img alt=\"Creative Commons License\" style=\"border-width:0;vertical-align:middle;float:right\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" />\n    </a>\n</p>\n"}]},"apps":[],"jobName":"paragraph_1575071143805_-1607596606","id":"20160720-131940_474698556","dateCreated":"2019-11-29T20:45:43-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:11469"},{"text":"%md\n# Introducción a Spark\n","user":"anonymous","dateUpdated":"2019-11-29T20:45:43-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Introducción a Spark</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1575071143815_-822678058","id":"20160628-160644_98292392","dateCreated":"2019-11-29T20:45:43-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11470"},{"text":"%md\n## Características\n\n### 100x más rápido que Hadoop MapReduce en memoria.\n### 10x más rápido en disco.\n  ![](https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spark_speed.png)\n","user":"anonymous","dateUpdated":"2019-12-12T01:01:06-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Características</h2>\n<h3>100x más rápido que Hadoop MapReduce en memoria.</h3>\n<h3>10x más rápido en disco.</h3>\n<p><img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spark_speed.png\" /></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575071143817_1840573278","id":"20171013-102503_1459120534","dateCreated":"2019-11-29T20:45:43-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11471"},{"text":"%md\n### Multiplataforma\n\n* Corre en Hadoop Yarn, Mesos, standalone o en la nube (AWS, Azure, ...)\n* Acceso a datos en HDFS, Cassandra, HBase, Hive, Tachyon, JDBC, etc.\n![](https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spark_multi_plataforma.png)\n","user":"anonymous","dateUpdated":"2019-12-12T01:01:10-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Multiplataforma</h3>\n<ul>\n  <li>Corre en Hadoop Yarn, Mesos, standalone o en la nube (AWS, Azure, &hellip;)</li>\n  <li>Acceso a datos en HDFS, Cassandra, HBase, Hive, Tachyon, JDBC, etc.<br/><img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spark_multi_plataforma.png\" /></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1575071143818_638703371","id":"20171013-105605_1380652694","dateCreated":"2019-11-29T20:45:43-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11472"},{"text":"%md\n### +50 empresas.\n\n### +200 desarrolladores.\n![](https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spar_contribs.png)\n","user":"anonymous","dateUpdated":"2019-12-12T01:01:18-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>+50 empresas.</h3>\n<h3>+200 desarrolladores.</h3>\n<p><img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spar_contribs.png\" /></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575071143819_91159354","id":"20171013-112527_2104876012","dateCreated":"2019-11-29T20:45:43-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11473"},{"title":"Múltiples funcionalidades en una plataforma (Stack unificado)","text":"print(s\"\"\"%html\n<img src=\"$baseDir/01_intro_spark/unified_stack.png\" alt=\"Drawing\" style=\"width: 60%;\"/>\n\"\"\")\n","user":"anonymous","dateUpdated":"2019-12-12T01:01:21-0300","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"fontSize":9,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/unified_stack.png\" alt=\"Drawing\" style=\"width: 60%;\"/>\n"}]},"apps":[],"jobName":"paragraph_1575071143820_-132065513","id":"20171013-110124_1830702370","dateCreated":"2019-11-29T20:45:43-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11474"},{"text":"%md\n## Fácil de usar\n\n* Interface de programación en Scala, Java, Python y R.\n* Notebooks: Zeppelin, Jupiter, ...","user":"anonymous","dateUpdated":"2019-11-29T20:45:43-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Fácil de usar</h2>\n<ul>\n  <li>Interface de programación en Scala, Java, Python y R.</li>\n  <li>Notebooks: Zeppelin, Jupiter, &hellip;</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1575071143821_-1533151487","id":"20171013-111851_1940139005","dateCreated":"2019-11-29T20:45:43-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11475"},{"title":"Word Count (MapReduce)","text":"%md\n```java\npublic class WordCount {\n\n\tpublic static class Map extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable> {\n\n\t\tprivate final static IntWritable one = new IntWritable(1);\n\n\t\tprivate Text word = new Text();\n\n\t\tpublic void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {\n\n\t\t\tString line = value.toString();\n\n\t\t\tStringTokenizer tokenizer = new StringTokenizer(line);\n\n\t\t\twhile (tokenizer.hasMoreTokens()) {\n\n\t\t\t\tword.set(tokenizer.nextToken());\n\n\t\t\t\toutput.collect(word, one);\n\n\t\t\t}\n\n\t\t}\n\n\t}\n\n\tpublic static class Reduce extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> {\n\n\t\tpublic void reduce(Text key, Iterator values, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {\n\n\t\t\tint sum = 0;\n\n\t\t\twhile (values.hasNext()) {\n\n\t\t\t\tsum += values.next().get();\n\n\t\t\t}\n\n\t\t\toutput.collect(key, new IntWritable(sum));\n\n\t\t}\n\n\t}\n\n\tpublic static void main(String[] args) throws Exception {\n\n\t\tJobConf conf = new JobConf(WordCount.class);\n\n\t\tconf.setJobName(\"wordcount\");\n\n\t\tconf.setOutputKeyClass(Text.class);\n\n\t\tconf.setOutputValueClass(IntWritable.class);\n\n\t\tconf.setMapperClass(Map.class);\n\n\t\tconf.setCombinerClass(Reduce.class);\n\n\t\tconf.setReducerClass(Reduce.class);\n\n\t\tconf.setInputFormat(TextInputFormat.class);\n\n\t\tconf.setOutputFormat(TextOutputFormat.class);\n\n\t\tFileInputFormat.setInputPaths(conf, new Path(args[0]));\n\n\t\tFileOutputFormat.setOutputPath(conf, new Path(args[1]));\n\n\t\tJobClient.runJob(conf);\n\n\t}\n\n}\n```\n\n\n","user":"anonymous","dateUpdated":"2019-11-29T21:09:00-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":{},"enabled":false,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<pre><code class=\"java\">public class WordCount {\n\n\tpublic static class Map extends MapReduceBase implements Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {\n\n\t\tprivate final static IntWritable one = new IntWritable(1);\n\n\t\tprivate Text word = new Text();\n\n\t\tpublic void map(LongWritable key, Text value, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter) throws IOException {\n\n\t\t\tString line = value.toString();\n\n\t\t\tStringTokenizer tokenizer = new StringTokenizer(line);\n\n\t\t\twhile (tokenizer.hasMoreTokens()) {\n\n\t\t\t\tword.set(tokenizer.nextToken());\n\n\t\t\t\toutput.collect(word, one);\n\n\t\t\t}\n\n\t\t}\n\n\t}\n\n\tpublic static class Reduce extends MapReduceBase implements Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {\n\n\t\tpublic void reduce(Text key, Iterator values, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter) throws IOException {\n\n\t\t\tint sum = 0;\n\n\t\t\twhile (values.hasNext()) {\n\n\t\t\t\tsum += values.next().get();\n\n\t\t\t}\n\n\t\t\toutput.collect(key, new IntWritable(sum));\n\n\t\t}\n\n\t}\n\n\tpublic static void main(String[] args) throws Exception {\n\n\t\tJobConf conf = new JobConf(WordCount.class);\n\n\t\tconf.setJobName(&quot;wordcount&quot;);\n\n\t\tconf.setOutputKeyClass(Text.class);\n\n\t\tconf.setOutputValueClass(IntWritable.class);\n\n\t\tconf.setMapperClass(Map.class);\n\n\t\tconf.setCombinerClass(Reduce.class);\n\n\t\tconf.setReducerClass(Reduce.class);\n\n\t\tconf.setInputFormat(TextInputFormat.class);\n\n\t\tconf.setOutputFormat(TextOutputFormat.class);\n\n\t\tFileInputFormat.setInputPaths(conf, new Path(args[0]));\n\n\t\tFileOutputFormat.setOutputPath(conf, new Path(args[1]));\n\n\t\tJobClient.runJob(conf);\n\n\t}\n\n}\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1575071143822_1339991866","id":"20171011-151944_1744659917","dateCreated":"2019-11-29T20:45:43-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11476"},{"title":"Word Count (Spark)","text":"%pyspark\n\nlines = sc.textFile(\"README.md\")\n\nwords = lines \\\n    .flatMap(lambda line: line.split(\" \")) \\\n    .filter(lambda word: word)\n\n#MapReduce\nwordCount = words \\\n    .map(lambda word: (word,1)) \\\n    .reduceByKey(lambda n,m: n+m)\n    ","user":"anonymous","dateUpdated":"2019-12-09T22:32:22-0300","config":{"lineNumbers":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/python","editorHide":false,"fontSize":13,"title":true,"results":{},"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1575071143823_-730772367","id":"20171010-190559_1468582504","dateCreated":"2019-11-29T20:45:43-0300","dateStarted":"2019-12-09T22:32:22-0300","dateFinished":"2019-12-09T22:32:59-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11477"},{"title":"Un poco de Scala","text":"%md\n\n* `lines` es un **array distribuido** de lineas de texto (`RDD[str]`).\n    - una parte del arreglo en cada **nodo del cluster**.\n\n* `lines` tiene el método `flatMap` (línea 6):\n    - `flatMap(lambda line: line.split(\" \"))` toma cada cada elemento del `RDD` (linea), lo convierte en sequencia de palabras y concatena estas secuencias:\n        - `lambda line: line.split(\" \")` es la **función** que toma una linea y la divide en una secuencia de palabras.\n        \n    - Su resultado es un array **distribuido** de palabras (`RDD[str]`).\n    \n* Al resultado de `flatMap` se aplica el método `filter` (línea 7):\n    - `filter(lambda word: word)` saca las palabras que son vacías (pueden aparecer?).\n    - `lambda word: word` es la **función** que pregunta si la palabra es vacía.\n    - `filter` devuelve un `RDD` que se almacena en `words`.\n\n* `words` tiene el método `map` (línea 11):\n    - `map(lambda word: (word,1))` agrega a cada palabra de `words` un `1`.\n    - El resultado es un **arreglo distribuido** de tuplas `RDD[(str, Int)]`.\n    \n* A este `RDD` se le aplica el método `reduceByKey` (línea 12):\n    - `reduceByKey(lambda n,m: n+m)` suma los `1`'s de las palabras iguales (la key es la palabra).\n\n","user":"anonymous","dateUpdated":"2019-11-29T20:45:43-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"title":false,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>\n    <p><code>lines</code> es un <strong>array distribuido</strong> de lineas de texto (<code>RDD[str]</code>).</p>\n    <ul>\n      <li>una parte del arreglo en cada <strong>nodo del cluster</strong>.</li>\n    </ul>\n  </li>\n  <li>\n    <p><code>lines</code> tiene el método <code>flatMap</code> (línea 6):</p>\n    <ul>\n      <li>\n        <p><code>flatMap(lambda line: line.split(&quot; &quot;))</code> toma cada cada elemento del <code>RDD</code> (linea), lo convierte en sequencia de palabras y concatena estas secuencias:</p>\n        <ul>\n          <li><code>lambda line: line.split(&quot; &quot;)</code> es la <strong>función</strong> que toma una linea y la divide en una secuencia de palabras.</li>\n        </ul>\n      </li>\n      <li>\n      <p>Su resultado es un array <strong>distribuido</strong> de palabras (<code>RDD[str]</code>).</p></li>\n    </ul>\n  </li>\n  <li>\n    <p>Al resultado de <code>flatMap</code> se aplica el método <code>filter</code> (línea 7):</p>\n    <ul>\n      <li><code>filter(lambda word: word)</code> saca las palabras que son vacías (pueden aparecer?).</li>\n      <li><code>lambda word: word</code> es la <strong>función</strong> que pregunta si la palabra es vacía.</li>\n      <li><code>filter</code> devuelve un <code>RDD</code> que se almacena en <code>words</code>.</li>\n    </ul>\n  </li>\n  <li>\n    <p><code>words</code> tiene el método <code>map</code> (línea 11):</p>\n    <ul>\n      <li><code>map(lambda word: (word,1))</code> agrega a cada palabra de <code>words</code> un <code>1</code>.</li>\n      <li>El resultado es un <strong>arreglo distribuido</strong> de tuplas <code>RDD[(str, Int)]</code>.</li>\n    </ul>\n  </li>\n  <li>\n    <p>A este <code>RDD</code> se le aplica el método <code>reduceByKey</code> (línea 12):</p>\n    <ul>\n      <li><code>reduceByKey(lambda n,m: n+m)</code> suma los <code>1</code>&rsquo;s de las palabras iguales (la key es la palabra).</li>\n    </ul>\n  </li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1575071143824_1653844601","id":"20181010-120216_1622145406","dateCreated":"2019-11-29T20:45:43-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11478"},{"title":"Resultado Word Count Spark","text":"%pyspark\n\nresult = wordCount \\\n    .sortBy((lambda p: p[1]), ascending = False) # ordena por cantidad\n\nlocal_result = result.collect() # Traigo desde cluster\n\nfor word, count in local_result[:10]: # tomo 10\n    print(word, count) # los imprimo\n","user":"anonymous","dateUpdated":"2019-12-09T22:33:45-0300","config":{"lineNumbers":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(u'from', 4)\n(u'and', 3)\n(u'Apache', 3)\n(u'to', 3)\n(u'Zeppelin', 3)\n(u'Please', 2)\n(u'###', 2)\n(u'interactive', 2)\n(u'*', 2)\n(u'binary', 2)\n"}]},"apps":[],"jobName":"paragraph_1575071143825_-1716692199","id":"20171011-153126_91229243","dateCreated":"2019-11-29T20:45:43-0300","dateStarted":"2019-12-09T22:33:46-0300","dateFinished":"2019-12-09T22:33:46-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11479"},{"title":"Run this","text":"%pyspark\n\nuiHost = sc.getConf().get(\"spark.driver.host\")#.getOrElse(\"localhost\")\nuiPort = sc.uiWebUrl.split(\":\")[-1]\nprint \"\"\"%html\nEjecutar esta celda y ver Spark UI en \n<a href=\"http://{}:{}\">http://{}(host):{}(port)</a>\n\"\"\".format(uiHost,uiPort,uiHost,uiPort)\n","user":"anonymous","dateUpdated":"2019-12-09T22:33:41-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"fontSize":9,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"Ejecutar esta celda y ver Spark UI en \n<a href=\"http://192.168.100.3:4040\">http://192.168.100.3(host):4040(port)</a>\n\n"}]},"apps":[],"jobName":"paragraph_1575071143826_-2011601965","id":"20171010-193244_2031028749","dateCreated":"2019-11-29T20:45:43-0300","dateStarted":"2019-12-09T22:33:41-0300","dateFinished":"2019-12-09T22:33:41-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11480"},{"title":"Ejercicio 0 (word count)","text":"%md\n* Crear una celda abajo de esta (poner mouse debajo de esta celda y seleccionar \"Add Paragraph\").\n* Copiar el programa `wordcount` anterior en la misma (esta en 2 celdas).\n    - [`shift`]-[`flechas`] para seleccionar.\n    - [`ctrl`]-[`c`] para copiar.\n    - [`ctrl`]-[`v`] para pegar.\n* Modificarlo para leer todas la lineas de los archivos en `./licenses/`\n    - Ayuda: si al método `textFile` se le indica el nombre de un directorio carga todos los archivo del mismo.\n* Ejecute la celda ([`shift`]-[`enter`])\n* Ver la cantidad de tareas en SparkUI\n\n","user":"anonymous","dateUpdated":"2019-11-29T20:45:43-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>Crear una celda abajo de esta (poner mouse debajo de esta celda y seleccionar &ldquo;Add Paragraph&rdquo;).</li>\n  <li>Copiar el programa <code>wordcount</code> anterior en la misma (esta en 2 celdas).\n    <ul>\n      <li>[<code>shift</code>]-[<code>flechas</code>] para seleccionar.</li>\n      <li>[<code>ctrl</code>]-[<code>c</code>] para copiar.</li>\n      <li>[<code>ctrl</code>]-[<code>v</code>] para pegar.</li>\n    </ul>\n  </li>\n  <li>Modificarlo para leer todas la lineas de los archivos en <code>./licenses/</code>\n    <ul>\n      <li>Ayuda: si al método <code>textFile</code> se le indica el nombre de un directorio carga todos los archivo del mismo.</li>\n    </ul>\n  </li>\n  <li>Ejecute la celda ([<code>shift</code>]-[<code>enter</code>])</li>\n  <li>Ver la cantidad de tareas en SparkUI</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1575071143827_1892927873","id":"20171010-205347_2087717007","dateCreated":"2019-11-29T20:45:43-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11481"},{"text":"%pyspark\n\nlines = sc.textFile(\"./licenses\")\n\nwords = lines \\\n    .flatMap(lambda line: line.split(\" \")) \\\n    .filter(lambda word: word)\n\n#MapReduce\nwordCount = words \\\n    .map(lambda word: (word,1)) \\\n    .reduceByKey(lambda n,m: n+m)\n    \n","user":"anonymous","dateUpdated":"2019-12-09T22:33:50-0300","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1575074014228_1923999979","id":"20191129-213334_2132577975","dateCreated":"2019-11-29T21:33:34-0300","dateStarted":"2019-12-09T22:33:50-0300","dateFinished":"2019-12-09T22:33:51-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11482"},{"text":"%pyspark\n\nresult = wordCount \\\n    .sortBy((lambda p: p[1]), ascending = False) # ordena por cantidad\n\nlocal_result = result.collect() # Traigo desde cluster\n\nfor word, count in local_result[:10]: # tomo 10\n    print(word, count) # los imprimo","user":"anonymous","dateUpdated":"2019-12-09T22:33:52-0300","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(u'the', 1098)\n(u'of', 687)\n(u'to', 567)\n(u'or', 499)\n(u'and', 473)\n(u'OR', 419)\n(u'OF', 324)\n(u'THE', 316)\n(u'in', 289)\n(u'any', 257)\n"}]},"apps":[],"jobName":"paragraph_1575074129527_-1574913962","id":"20191129-213529_1587127863","dateCreated":"2019-11-29T21:35:29-0300","dateStarted":"2019-12-09T22:33:52-0300","dateFinished":"2019-12-09T22:34:08-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11483"},{"text":"%md\n\n## Ejecución de programas en Spark\n\n* En [Zeppelin](http://zeppelin.apache.org/) (como lo hacemos ahora)\n* En `pyspark` shell (tambien interactivo)\n* Como programa autónomo\n","user":"anonymous","dateUpdated":"2019-11-29T20:45:43-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Ejecución de programas en Spark</h2>\n<ul>\n  <li>En <a href=\"http://zeppelin.apache.org/\">Zeppelin</a> (como lo hacemos ahora)</li>\n  <li>En <code>pyspark</code> shell</li>\n  <li>Como programa autónomo</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1575071143828_-1360830307","id":"20171010-202757_196880209","dateCreated":"2019-11-29T20:45:43-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11484"},{"title":"pyspark shell","text":"%md\n\n* Abrir una terminal\n* Ir a la instalación Spark\n```sh\ncd ~/spark/spark-2.2.1-bin-hadoop2.7\n```\n* Arrancar el shell\n```sh\n./bin/pyspark\n```\n* Escribir en shell (después apretar `Enter`)\n```python\n>>> lines = sc.textFile(\"README.md\")\n>>> textFile.first()\n```\n","user":"anonymous","dateUpdated":"2019-11-29T20:45:43-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>Abrir una terminal</li>\n  <li>\n  <p>Ir a la instalación Spark</p>\n  <pre><code class=\"sh\">cd ~/spark/spark-2.2.1-bin-hadoop2.7\n</code></pre></li>\n  <li>\n  <p>Arrancar el shell</p>\n  <pre><code class=\"sh\">./bin/pyspark\n</code></pre></li>\n  <li>\n  <p>Escribir en shell (después apretar <code>Enter</code>)</p>\n  <pre><code class=\"python\">&gt;&gt;&gt; lines = sc.textFile(&quot;README.md&quot;)\n&gt;&gt;&gt; textFile.first()\n</code></pre></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1575071143829_781282981","id":"20171011-173126_528319238","dateCreated":"2019-11-29T20:45:43-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11485"},{"title":"Programa autónomo","text":"%md\n\n* Ir a programa\n```sh\ncd diplodatos_bigdata/prog/word_count\n```\n* Actualizar repositorio\n```sh\ngit pull\n```\n* Ver programa\n```sh\nless src/main/python/WordCount.py\n```\n  (salir con [`q`])\n","user":"anonymous","dateUpdated":"2019-11-29T20:45:43-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>\n  <p>Ir a programa</p>\n  <pre><code class=\"sh\">cd diplodatos_bigdata/prog/word_count\n</code></pre></li>\n  <li>\n  <p>Actualizar repositorio</p>\n  <pre><code class=\"sh\">git pull\n</code></pre></li>\n  <li>\n  <p>Ver programa</p>\n  <pre><code class=\"sh\">less src/main/python/WordCount.py\n</code></pre> (salir con [<code>q</code>])</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1575071143830_-1804109369","id":"20171011-175259_1199949339","dateCreated":"2019-11-29T20:45:43-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11486"},{"title":"Ejecucion de programa","text":"%md\n* Ejecutar\n```sh\n~/spark/spark-2.2.1-bin-hadoop2.7/bin/spark-submit --master local[4] src/main/python/WordCount.py \\\n        ~/spark/spark-2.2.1-bin-hadoop2.7/licenses/\n```\n","user":"anonymous","dateUpdated":"2019-11-29T20:45:43-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>\n  <p>Ejecutar</p>\n  <pre><code class=\"sh\">~/spark/spark-2.2.1-bin-hadoop2.7/bin/spark-submit --master local[4] src/main/python/WordCount.py \\\n    ~/spark/spark-2.2.1-bin-hadoop2.7/licenses/\n</code></pre></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1575071143831_-393778678","id":"20171012-165049_905215351","dateCreated":"2019-11-29T20:45:43-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11487"},{"title":"Versión Spark en Zeppelin","text":"%pyspark\n\nprint(sc.version)","user":"anonymous","dateUpdated":"2019-12-09T22:34:28-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"2.2.1\n"}]},"apps":[],"jobName":"paragraph_1575071143832_701439908","id":"20170830-114757_1684133948","dateCreated":"2019-11-29T20:45:43-0300","dateStarted":"2019-12-09T22:34:28-0300","dateFinished":"2019-12-09T22:34:28-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11488"},{"text":"%md\n\n### Principales referencias online:\n\n* [Documentación Spark](http://spark.apache.org/docs/2.2.1/)\n* [API Spark Python](http://spark.apache.org/docs/2.2.1/api/python/index.html)\n","user":"anonymous","dateUpdated":"2019-11-29T20:45:43-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Principales referencias online:</h3>\n<ul>\n  <li><a href=\"http://spark.apache.org/docs/2.2.1/\">Documentación Spark</a></li>\n  <li><a href=\"http://spark.apache.org/docs/2.2.1/api/python/index.html\">API Spark Python</a></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1575071143834_1349002280","id":"20181012-171203_1400816125","dateCreated":"2019-11-29T20:45:43-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11489"},{"text":"%md\n## Ejercicios MapReduce con Spark","user":"anonymous","dateUpdated":"2019-11-29T20:45:43-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Ejercicios MapReduce con Spark</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1575071143835_335599515","id":"20171016-172908_1510165702","dateCreated":"2019-11-29T20:45:43-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11490"},{"title":"Ejercicio 1","text":"%md\n\nModifique el programa *word count* siguiente para que cuente la **cantidad de apariciones de cada letra** en el archivo.\n\n* Ayuda: solo hay que modificar la linea 6\n","user":"anonymous","dateUpdated":"2019-11-29T20:45:43-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Modifique el programa <em>word count</em> siguiente para que cuente la <strong>cantidad de apariciones de cada letra</strong> en el archivo.</p>\n<ul>\n  <li>Ayuda: solo hay que modificar la linea 6</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1575071143836_-1804428300","id":"20171010-202446_178633207","dateCreated":"2019-11-29T20:45:43-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11491"},{"text":"%pyspark\n\nlines = sc.textFile(\"README.md\")\n\nwords = lines \\\n    .flatMap(lambda line: line.split(\" \")) \\\n    .filter(lambda word: word)\n\n\n#MapReduce\nwordCount = words \\\n    .map(lambda word: (word,1)) \\\n    .reduceByKey(lambda n,m: n+m)\n\nresult = wordCount \\\n    .sortBy((lambda p: p[1]), ascending = False) # ordena por cantidad\n\nlocal_result = result.collect() # Traigo desde cluster\n\nfor word, count in local_result[:10]: # tomo 10\n    print word, count # los imprimo\n","user":"anonymous","dateUpdated":"2019-12-09T22:34:33-0300","config":{"lineNumbers":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"from 4\nand 3\nApache 3\nto 3\nZeppelin 3\nPlease 2\n### 2\ninteractive 2\n* 2\nbinary 2\n"}]},"apps":[],"jobName":"paragraph_1575071143837_-16438088","id":"20171011-154135_2131718224","dateCreated":"2019-11-29T20:45:43-0300","dateStarted":"2019-12-09T22:34:33-0300","dateFinished":"2019-12-09T22:34:33-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11492"},{"title":"Ejercicio 2","text":"%md\nCada línea del archivo `~/diplodatos_bigdata/ds/links_raw.txt` contiene un url de una página web seguido de los links que posee a otras páginas web:\n```\n<url> <url link 1> <url link 2> ... <url link n>\n```\n\nBasándose en la utilización de la técnica de *MapReduce* que se mostró en el programa `word count` haga un programa en Spark que cuente la cantidad de links que apuntan a cada página.\n\n#### Ayuda\n\nA continuación está el comienzo del programa. Falta hacer el *MapReduce* y mostrar el resultado.\n","user":"anonymous","dateUpdated":"2019-11-29T20:45:43-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":{},"enabled":false,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Cada línea del archivo <code>~/diplodatos_bigdata/ds/links_raw.txt</code> contiene un url de una página web seguido de los links que posee a otras páginas web:</p>\n<pre><code>&lt;url&gt; &lt;url link 1&gt; &lt;url link 2&gt; ... &lt;url link n&gt;\n</code></pre>\n<p>Basándose en la utilización de la técnica de <em>MapReduce</em> que se mostró en el programa <code>word count</code> haga un programa en Spark que cuente la cantidad de links que apuntan a cada página.</p>\n<h4>Ayuda</h4>\n<p>A continuación está el comienzo del programa. Falta hacer el <em>MapReduce</em> y mostrar el resultado.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575071143837_1601786247","id":"20171011-175322_1451259292","dateCreated":"2019-11-29T20:45:43-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11493"},{"text":"%pyspark\n\nbaseDir = \"../..\" # llenar con el directorio git\n\nlines = sc.textFile(baseDir + \"/ds/links_raw.txt\")\n\nlinksTo = lines \\\n    .flatMap(lambda l: l.split(\" \")[1:]) # separo los links y tomo los apuntados\n\n# Ahora linksTo tiene las paginas apuntadas\n\n# Completar los ...\n\n# MapReduce\ninvLinkCount = linksTo.map(lambda link: (link,1)) \\\n    .reduceByKey(lambda n,m: n+m)\n\nresult = invLinkCount.sortBy((lambda p: p[1]), ascending = False)\n\nlocal_result = result.collect() # Traigo desde cluster\n\nfor word, count in local_result[:10]: # tomo 10\n    print word, count # los imprimo\n","user":"anonymous","dateUpdated":"2019-12-09T22:40:00-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"http://www.yahoo.com/ 199\nhttp://www.ca.gov/ 169\nhttp://www.leginfo.ca.gov/calaw.html 155\nhttp://www.linkexchange.com/ 134\nhttp://www.berkeley.edu/ 126\nhttp://www.sen.ca.gov/ 123\nhttp://home.netscape.com/comprod/mirror/index.html 109\nhttp://www.assembly.ca.gov/ 99\nhttp://www.epa.gov/ 95\nhttp://www.usgs.gov/ 84\n"}]},"apps":[],"jobName":"paragraph_1575071143838_-910305041","id":"20191121-184701_1405603118","dateCreated":"2019-11-29T20:45:43-0300","dateStarted":"2019-12-09T22:40:00-0300","dateFinished":"2019-12-09T22:40:01-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11494"},{"title":"FIN","text":"//val baseDir=\"https://git.cs.famaf.unc.edu.ar/dbarsotti/diplodatos_bigdata/raw/master/clases\"\nval baseDir=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases\"\n\nz.put(\"baseDir\", baseDir)\nprint(\"\"\"%html\n<script>\n    var heads = document.getElementsByTagName('h2');\n    var numHeads = heads.length;\n    var inner = \"\";\n    var i = 0;\n    var j = 0;\n    while (i < numHeads){\n        inner = heads[i].innerHTML;\n        if (inner.search(\".-\") != -1 ) {\n            j++;\n            heads[i].innerHTML = inner.replace(/(~|\\d+)\\.-/,\"\"+j+\".-\");\n        }\n        i++\n    }\n</script>\n\"\"\")","user":"anonymous","dateUpdated":"2019-11-29T20:45:43-0300","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"fontSize":9,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<script>\n    var heads = document.getElementsByTagName('h2');\n    var numHeads = heads.length;\n    var inner = \"\";\n    var i = 0;\n    var j = 0;\n    while (i < numHeads){\n        inner = heads[i].innerHTML;\n        if (inner.search(\".-\") != -1 ) {\n            j++;\n            heads[i].innerHTML = inner.replace(/(~|\\d+)\\.-/,\"\"+j+\".-\");\n        }\n        i++\n    }\n</script>\nbaseDir: String = https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases\n"}]},"apps":[],"jobName":"paragraph_1575071143839_1060613402","id":"20160712-175904_2058049512","dateCreated":"2019-11-29T20:45:43-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11495"},{"user":"anonymous","dateUpdated":"2019-11-29T20:45:43-0300","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575071143840_-282304005","id":"20191129-104030_1246831484","dateCreated":"2019-11-29T20:45:43-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11496"}],"name":"Diplodatos/Clase 01 - Introducción a Spark","id":"2ESZKWWJ3","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}