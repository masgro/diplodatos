{
  "paragraphs": [
    {
      "text": "print(s\"\"\"%html\n\u003ccenter\u003e\n    \u003ch1\u003e\u003ca href\u003d\"http://diplodatos.famaf.unc.edu.ar/\"\u003eDiplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones\u003c/a\u003e\u003c/h1\u003e\n    \u003ch2\u003eCurso \u003ca href\u003d\"https://sites.google.com/view/eleccion-optativas-diplodatos/programaci%C3%B3n-distribu%C3%ADda-sobre-grandes-vol%C3%BAmenes-de-datos\"\u003eProgramación Distribuida sobre Grandes Volúmenes de Datos\u003c/a\u003e\u003c/h2\u003e\n\u003c/center\u003e\n\n\u003cbr\u003e\n\n\u003ch3 style\u003d\"text-align:center;\"\u003e Damián Barsotti  \u003c/h3\u003e\n\n\u003ch3 style\u003d\"text-align:center;\"\u003e\n    \u003ca href\u003d\"http://www.famaf.unc.edu.ar\"\u003e\n    Facultad de Matemática Astronomía Física y Computación\n    \u003c/a\u003e\n\u003cbr/\u003e\n    \u003ca href\u003d\"http://www.unc.edu.ar\"\u003e\n    Universidad Nacional de Córdoba\n    \u003c/a\u003e\n\u003cbr/\u003e\n    \u003ccenter\u003e\n    \u003ca href\u003d\"http://www.famaf.unc.edu.ar\"\u003e\n    \u003cimg src\u003d\"$baseDir/comun/logo%20UNC%20FAMAF%202016.png\" alt\u003d\"Drawing\" style\u003d\"width:50%;\"/\u003e\n    \u003c/a\u003e\n    \u003c/center\u003e\n\u003c/h3\u003e\n\n\u003cp style\u003d\"font-size:15px;\"\u003e\n    \u003cbr /\u003e\n        This work is licensed under a\n        \u003ca rel\u003d\"license\" href\u003d\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"\u003eCreative Commons Attribution-NonCommercial-ShareAlike 4.0 International License\u003c/a\u003e.\n    \u003ca rel\u003d\"license\" href\u003d\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"\u003e\n        \u003cimg alt\u003d\"Creative Commons License\" style\u003d\"border-width:0;vertical-align:middle;float:right\" src\u003d\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /\u003e\n    \u003c/a\u003e\n\u003c/p\u003e\n\"\"\")\n",
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:02:27.886",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "fontSize": 9.0,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ccenter\u003e\n    \u003ch1\u003e\u003ca href\u003d\"http://diplodatos.famaf.unc.edu.ar/\"\u003eDiplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones\u003c/a\u003e\u003c/h1\u003e\n    \u003ch2\u003eCurso \u003ca href\u003d\"https://sites.google.com/view/eleccion-optativas-diplodatos/programaci%C3%B3n-distribu%C3%ADda-sobre-grandes-vol%C3%BAmenes-de-datos\"\u003eProgramación Distribuida sobre Grandes Volúmenes de Datos\u003c/a\u003e\u003c/h2\u003e\n\u003c/center\u003e\n\n\u003cbr\u003e\n\n\u003ch3 style\u003d\"text-align:center;\"\u003e Damián Barsotti  \u003c/h3\u003e\n\n\u003ch3 style\u003d\"text-align:center;\"\u003e\n    \u003ca href\u003d\"http://www.famaf.unc.edu.ar\"\u003e\n    Facultad de Matemática Astronomía Física y Computación\n    \u003c/a\u003e\n\u003cbr/\u003e\n    \u003ca href\u003d\"http://www.unc.edu.ar\"\u003e\n    Universidad Nacional de Córdoba\n    \u003c/a\u003e\n\u003cbr/\u003e\n    \u003ccenter\u003e\n    \u003ca href\u003d\"http://www.famaf.unc.edu.ar\"\u003e\n    \u003cimg src\u003d\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/comun/logo%20UNC%20FAMAF%202016.png\" alt\u003d\"Drawing\" style\u003d\"width:50%;\"/\u003e\n    \u003c/a\u003e\n    \u003c/center\u003e\n\u003c/h3\u003e\n\n\u003cp style\u003d\"font-size:15px;\"\u003e\n    \u003cbr /\u003e\n        This work is licensed under a\n        \u003ca rel\u003d\"license\" href\u003d\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"\u003eCreative Commons Attribution-NonCommercial-ShareAlike 4.0 International License\u003c/a\u003e.\n    \u003ca rel\u003d\"license\" href\u003d\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"\u003e\n        \u003cimg alt\u003d\"Creative Commons License\" style\u003d\"border-width:0;vertical-align:middle;float:right\" src\u003d\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /\u003e\n    \u003c/a\u003e\n\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575082947882_937616596",
      "id": "20160720-131940_474698556",
      "dateCreated": "2019-11-30 00:02:27.882",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n# Introducción a Spark\n",
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:02:27.890",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003eIntroducción a Spark\u003c/h1\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575082947888_-1182687637",
      "id": "20160628-160644_98292392",
      "dateCreated": "2019-11-30 00:02:27.888",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Características\n\n### 100x más rápido que Hadoop MapReduce en memoria.\n### 10x más rápido en disco.\n  ![](https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spark_speed.png)\n",
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:02:27.892",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eCaracterísticas\u003c/h2\u003e\n\u003ch3\u003e100x más rápido que Hadoop MapReduce en memoria.\u003c/h3\u003e\n\u003ch3\u003e10x más rápido en disco.\u003c/h3\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spark_speed.png\" /\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575082947891_379402473",
      "id": "20171013-102503_1459120534",
      "dateCreated": "2019-11-30 00:02:27.891",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Multiplataforma\n\n* Corre en Hadoop Yarn, Mesos, standalone o en la nube (AWS, Azure, ...)\n* Acceso a datos en HDFS, Cassandra, HBase, Hive, Tachyon, JDBC, etc.\n![](https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spark_multi_plataforma.png)\n",
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:02:27.895",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eMultiplataforma\u003c/h3\u003e\n\u003cul\u003e\n  \u003cli\u003eCorre en Hadoop Yarn, Mesos, standalone o en la nube (AWS, Azure, \u0026hellip;)\u003c/li\u003e\n  \u003cli\u003eAcceso a datos en HDFS, Cassandra, HBase, Hive, Tachyon, JDBC, etc.\u003cbr/\u003e\u003cimg src\u003d\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spark_multi_plataforma.png\" /\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575082947893_795852193",
      "id": "20171013-105605_1380652694",
      "dateCreated": "2019-11-30 00:02:27.893",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### +50 empresas.\n\n### +200 desarrolladores.\n![](https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spar_contribs.png)\n",
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:02:27.897",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003e+50 empresas.\u003c/h3\u003e\n\u003ch3\u003e+200 desarrolladores.\u003c/h3\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spar_contribs.png\" /\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575082947896_-518105067",
      "id": "20171013-112527_2104876012",
      "dateCreated": "2019-11-30 00:02:27.896",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Múltiples funcionalidades en una plataforma (Stack unificado)",
      "text": "print(s\"\"\"%html\n\u003cimg src\u003d\"$baseDir/01_intro_spark/unified_stack.png\" alt\u003d\"Drawing\" style\u003d\"width: 60%;\"/\u003e\n\"\"\")\n",
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:02:27.900",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cimg src\u003d\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/unified_stack.png\" alt\u003d\"Drawing\" style\u003d\"width: 60%;\"/\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575082947898_752019326",
      "id": "20171013-110124_1830702370",
      "dateCreated": "2019-11-30 00:02:27.898",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Fácil de usar\n\n* Interface de programación en Scala, Java, Python y R.\n* Notebooks: Zeppelin, Jupiter, ...",
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:02:27.902",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eFácil de usar\u003c/h2\u003e\n\u003cul\u003e\n  \u003cli\u003eInterface de programación en Scala, Java, Python y R.\u003c/li\u003e\n  \u003cli\u003eNotebooks: Zeppelin, Jupiter, \u0026hellip;\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575082947900_1955385228",
      "id": "20171013-111851_1940139005",
      "dateCreated": "2019-11-30 00:02:27.900",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Word Count (MapReduce)",
      "text": "%md\n```java\npublic class WordCount {\n\n\tpublic static class Map extends MapReduceBase implements Mapper\u003cLongWritable, Text, Text, IntWritable\u003e {\n\n\t\tprivate final static IntWritable one \u003d new IntWritable(1);\n\n\t\tprivate Text word \u003d new Text();\n\n\t\tpublic void map(LongWritable key, Text value, OutputCollector\u003cText, IntWritable\u003e output, Reporter reporter) throws IOException {\n\n\t\t\tString line \u003d value.toString();\n\n\t\t\tStringTokenizer tokenizer \u003d new StringTokenizer(line);\n\n\t\t\twhile (tokenizer.hasMoreTokens()) {\n\n\t\t\t\tword.set(tokenizer.nextToken());\n\n\t\t\t\toutput.collect(word, one);\n\n\t\t\t}\n\n\t\t}\n\n\t}\n\n\tpublic static class Reduce extends MapReduceBase implements Reducer\u003cText, IntWritable, Text, IntWritable\u003e {\n\n\t\tpublic void reduce(Text key, Iterator values, OutputCollector\u003cText, IntWritable\u003e output, Reporter reporter) throws IOException {\n\n\t\t\tint sum \u003d 0;\n\n\t\t\twhile (values.hasNext()) {\n\n\t\t\t\tsum +\u003d values.next().get();\n\n\t\t\t}\n\n\t\t\toutput.collect(key, new IntWritable(sum));\n\n\t\t}\n\n\t}\n\n\tpublic static void main(String[] args) throws Exception {\n\n\t\tJobConf conf \u003d new JobConf(WordCount.class);\n\n\t\tconf.setJobName(\"wordcount\");\n\n\t\tconf.setOutputKeyClass(Text.class);\n\n\t\tconf.setOutputValueClass(IntWritable.class);\n\n\t\tconf.setMapperClass(Map.class);\n\n\t\tconf.setCombinerClass(Reduce.class);\n\n\t\tconf.setReducerClass(Reduce.class);\n\n\t\tconf.setInputFormat(TextInputFormat.class);\n\n\t\tconf.setOutputFormat(TextOutputFormat.class);\n\n\t\tFileInputFormat.setInputPaths(conf, new Path(args[0]));\n\n\t\tFileOutputFormat.setOutputPath(conf, new Path(args[1]));\n\n\t\tJobClient.runJob(conf);\n\n\t}\n\n}\n```\n\n",
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:02:27.904",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cpre\u003e\u003ccode class\u003d\"java\"\u003epublic class WordCount {\n\n\tpublic static class Map extends MapReduceBase implements Mapper\u0026lt;LongWritable, Text, Text, IntWritable\u0026gt; {\n\n\t\tprivate final static IntWritable one \u003d new IntWritable(1);\n\n\t\tprivate Text word \u003d new Text();\n\n\t\tpublic void map(LongWritable key, Text value, OutputCollector\u0026lt;Text, IntWritable\u0026gt; output, Reporter reporter) throws IOException {\n\n\t\t\tString line \u003d value.toString();\n\n\t\t\tStringTokenizer tokenizer \u003d new StringTokenizer(line);\n\n\t\t\twhile (tokenizer.hasMoreTokens()) {\n\n\t\t\t\tword.set(tokenizer.nextToken());\n\n\t\t\t\toutput.collect(word, one);\n\n\t\t\t}\n\n\t\t}\n\n\t}\n\n\tpublic static class Reduce extends MapReduceBase implements Reducer\u0026lt;Text, IntWritable, Text, IntWritable\u0026gt; {\n\n\t\tpublic void reduce(Text key, Iterator values, OutputCollector\u0026lt;Text, IntWritable\u0026gt; output, Reporter reporter) throws IOException {\n\n\t\t\tint sum \u003d 0;\n\n\t\t\twhile (values.hasNext()) {\n\n\t\t\t\tsum +\u003d values.next().get();\n\n\t\t\t}\n\n\t\t\toutput.collect(key, new IntWritable(sum));\n\n\t\t}\n\n\t}\n\n\tpublic static void main(String[] args) throws Exception {\n\n\t\tJobConf conf \u003d new JobConf(WordCount.class);\n\n\t\tconf.setJobName(\u0026quot;wordcount\u0026quot;);\n\n\t\tconf.setOutputKeyClass(Text.class);\n\n\t\tconf.setOutputValueClass(IntWritable.class);\n\n\t\tconf.setMapperClass(Map.class);\n\n\t\tconf.setCombinerClass(Reduce.class);\n\n\t\tconf.setReducerClass(Reduce.class);\n\n\t\tconf.setInputFormat(TextInputFormat.class);\n\n\t\tconf.setOutputFormat(TextOutputFormat.class);\n\n\t\tFileInputFormat.setInputPaths(conf, new Path(args[0]));\n\n\t\tFileOutputFormat.setOutputPath(conf, new Path(args[1]));\n\n\t\tJobClient.runJob(conf);\n\n\t}\n\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575082947903_1431977914",
      "id": "20171011-151944_1744659917",
      "dateCreated": "2019-11-30 00:02:27.903",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Word Count (Spark)",
      "text": "%pyspark\n\nlines \u003d sc.textFile(\"README.md\")\n\nwords \u003d lines \\\n    .flatMap(lambda line: line.split(\" \")) \\\n    .filter(lambda word: word)\n\n#MapReduce\nwordCount \u003d words \\\n    .map(lambda word: (word,1)) \\\n    .reduceByKey(lambda n,m: n+m)\n",
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:02:27.907",
      "config": {
        "lineNumbers": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1575082947905_-24180001",
      "id": "20171010-190559_1468582504",
      "dateCreated": "2019-11-30 00:02:27.905",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Un poco de Scala",
      "text": "%md\n\n* `lines` es un **array distribuido** de lineas de texto (`RDD[str]`).\n    - una parte del arreglo en cada **nodo del cluster**.\n\n* `lines` tiene el método `flatMap` (línea 6):\n    - `flatMap(lambda line: line.split(\" \"))` toma cada cada elemento del `RDD` (linea), lo convierte en sequencia de palabras y concatena estas secuencias:\n        - `lambda line: line.split(\" \")` es la **función** que toma una linea y la divide en una secuencia de palabras.\n        \n    - Su resultado es un array **distribuido** de palabras (`RDD[str]`).\n    \n* Al resultado de `flatMap` se aplica el método `filter` (línea 7):\n    - `filter(lambda word: word)` saca las palabras que son vacías (pueden aparecer?).\n    - `lambda word: word` es la **función** que pregunta si la palabra es vacía.\n    - `filter` devuelve un `RDD` que se almacena en `words`.\n\n* `words` tiene el método `map` (línea 11):\n    - `map(lambda word: (word,1))` agrega a cada palabra de `words` un `1`.\n    - El resultado es un **arreglo distribuido** de tuplas `RDD[(str, Int)]`.\n    \n* A este `RDD` se le aplica el método `reduceByKey` (línea 12):\n    - `reduceByKey(lambda n,m: n+m)` suma los `1`\u0027s de las palabras iguales (la key es la palabra).\n\n",
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:02:27.908",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "title": false,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003e\u003ccode\u003elines\u003c/code\u003e es un \u003cstrong\u003earray distribuido\u003c/strong\u003e de lineas de texto (\u003ccode\u003eRDD[str]\u003c/code\u003e).\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003euna parte del arreglo en cada \u003cstrong\u003enodo del cluster\u003c/strong\u003e.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e\u003ccode\u003elines\u003c/code\u003e tiene el método \u003ccode\u003eflatMap\u003c/code\u003e (línea 6):\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\n        \u003cp\u003e\u003ccode\u003eflatMap(lambda line: line.split(\u0026quot; \u0026quot;))\u003c/code\u003e toma cada cada elemento del \u003ccode\u003eRDD\u003c/code\u003e (linea), lo convierte en sequencia de palabras y concatena estas secuencias:\u003c/p\u003e\n        \u003cul\u003e\n          \u003cli\u003e\u003ccode\u003elambda line: line.split(\u0026quot; \u0026quot;)\u003c/code\u003e es la \u003cstrong\u003efunción\u003c/strong\u003e que toma una linea y la divide en una secuencia de palabras.\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\n      \u003cp\u003eSu resultado es un array \u003cstrong\u003edistribuido\u003c/strong\u003e de palabras (\u003ccode\u003eRDD[str]\u003c/code\u003e).\u003c/p\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eAl resultado de \u003ccode\u003eflatMap\u003c/code\u003e se aplica el método \u003ccode\u003efilter\u003c/code\u003e (línea 7):\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\u003ccode\u003efilter(lambda word: word)\u003c/code\u003e saca las palabras que son vacías (pueden aparecer?).\u003c/li\u003e\n      \u003cli\u003e\u003ccode\u003elambda word: word\u003c/code\u003e es la \u003cstrong\u003efunción\u003c/strong\u003e que pregunta si la palabra es vacía.\u003c/li\u003e\n      \u003cli\u003e\u003ccode\u003efilter\u003c/code\u003e devuelve un \u003ccode\u003eRDD\u003c/code\u003e que se almacena en \u003ccode\u003ewords\u003c/code\u003e.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e\u003ccode\u003ewords\u003c/code\u003e tiene el método \u003ccode\u003emap\u003c/code\u003e (línea 11):\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\u003ccode\u003emap(lambda word: (word,1))\u003c/code\u003e agrega a cada palabra de \u003ccode\u003ewords\u003c/code\u003e un \u003ccode\u003e1\u003c/code\u003e.\u003c/li\u003e\n      \u003cli\u003eEl resultado es un \u003cstrong\u003earreglo distribuido\u003c/strong\u003e de tuplas \u003ccode\u003eRDD[(str, Int)]\u003c/code\u003e.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eA este \u003ccode\u003eRDD\u003c/code\u003e se le aplica el método \u003ccode\u003ereduceByKey\u003c/code\u003e (línea 12):\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\u003ccode\u003ereduceByKey(lambda n,m: n+m)\u003c/code\u003e suma los \u003ccode\u003e1\u003c/code\u003e\u0026rsquo;s de las palabras iguales (la key es la palabra).\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575082947907_-1310776219",
      "id": "20181010-120216_1622145406",
      "dateCreated": "2019-11-30 00:02:27.907",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Resultado Word Count Spark",
      "text": "%pyspark\n\nresult \u003d wordCount \\\n    .sortBy((lambda p: p[1]), ascending \u003d False) # ordena por cantidad\n\nlocal_result \u003d result.collect() # Traigo desde cluster\n\nfor word, count in local_result[:10]: # tomo 10\n    print word, count # los imprimo\n",
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:02:27.910",
      "config": {
        "lineNumbers": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1575082947909_764128197",
      "id": "20171011-153126_91229243",
      "dateCreated": "2019-11-30 00:02:27.909",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Run this",
      "text": "%pyspark\n\nuiHost \u003d sc.getConf().get(\"spark.driver.host\")#.getOrElse(\"localhost\")\nuiPort \u003d sc.uiWebUrl.split(\":\")[-1]\nprint \"\"\"%html\nEjecutar esta celda y ver Spark UI en \n\u003ca href\u003d\"http://{}:{}\"\u003ehttp://{}(host):{}(port)\u003c/a\u003e\n\"\"\".format(uiHost,uiPort,uiHost,uiPort)\n",
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:02:27.913",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": true,
        "fontSize": 9.0,
        "title": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "Ejecutar esta celda y ver Spark UI en \n\u003ca href\u003d\"http://192.168.128.167:4040\"\u003ehttp://192.168.128.167(host):4040(port)\u003c/a\u003e\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575082947911_-499624364",
      "id": "20171010-193244_2031028749",
      "dateCreated": "2019-11-30 00:02:27.911",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Ejercicio 0 (word count)",
      "text": "%md\n* Crear una celda abajo de esta (poner mouse debajo de esta celda y seleccionar \"Add Paragraph\").\n* Copiar el programa `wordcount` anterior en la misma (esta en 2 celdas).\n    - [`shift`]-[`flechas`] para seleccionar.\n    - [`ctrl`]-[`c`] para copiar.\n    - [`ctrl`]-[`v`] para pegar.\n* Modificarlo para leer todas la lineas de los archivos en `./licenses/`\n    - Ayuda: si al método `textFile` se le indica el nombre de un directorio carga todos los archivo del mismo.\n* Ejecute la celda ([`shift`]-[`enter`])\n* Ver la cantidad de tareas en SparkUI\n\n",
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:02:27.915",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cul\u003e\n  \u003cli\u003eCrear una celda abajo de esta (poner mouse debajo de esta celda y seleccionar \u0026ldquo;Add Paragraph\u0026rdquo;).\u003c/li\u003e\n  \u003cli\u003eCopiar el programa \u003ccode\u003ewordcount\u003c/code\u003e anterior en la misma (esta en 2 celdas).\n    \u003cul\u003e\n      \u003cli\u003e[\u003ccode\u003eshift\u003c/code\u003e]-[\u003ccode\u003eflechas\u003c/code\u003e] para seleccionar.\u003c/li\u003e\n      \u003cli\u003e[\u003ccode\u003ectrl\u003c/code\u003e]-[\u003ccode\u003ec\u003c/code\u003e] para copiar.\u003c/li\u003e\n      \u003cli\u003e[\u003ccode\u003ectrl\u003c/code\u003e]-[\u003ccode\u003ev\u003c/code\u003e] para pegar.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003eModificarlo para leer todas la lineas de los archivos en \u003ccode\u003e./licenses/\u003c/code\u003e\n    \u003cul\u003e\n      \u003cli\u003eAyuda: si al método \u003ccode\u003etextFile\u003c/code\u003e se le indica el nombre de un directorio carga todos los archivo del mismo.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003eEjecute la celda ([\u003ccode\u003eshift\u003c/code\u003e]-[\u003ccode\u003eenter\u003c/code\u003e])\u003c/li\u003e\n  \u003cli\u003eVer la cantidad de tareas en SparkUI\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575082947913_-549446300",
      "id": "20171010-205347_2087717007",
      "dateCreated": "2019-11-30 00:02:27.913",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n## Ejecución de programas en Spark\n\n* En [Zeppelin](http://zeppelin.apache.org/) (como lo hacemos ahora)\n* En `pyspark` shell (tambien interactivo)\n* Como programa autónomo\n",
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:02:27.917",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eEjecución de programas en Spark\u003c/h2\u003e\n\u003cul\u003e\n  \u003cli\u003eEn \u003ca href\u003d\"http://zeppelin.apache.org/\"\u003eZeppelin\u003c/a\u003e (como lo hacemos ahora)\u003c/li\u003e\n  \u003cli\u003eEn \u003ccode\u003epyspark\u003c/code\u003e shell\u003c/li\u003e\n  \u003cli\u003eComo programa autónomo\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575082947915_-352394728",
      "id": "20171010-202757_196880209",
      "dateCreated": "2019-11-30 00:02:27.915",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "pyspark shell",
      "text": "%md\n\n* Abrir una terminal\n* Ir a la instalación Spark\n```sh\ncd ~/spark/spark-2.2.1-bin-hadoop2.7\n```\n* Arrancar el shell\n```sh\n./bin/pyspark\n```\n* Escribir en shell (después apretar `Enter`)\n```python\n\u003e\u003e\u003e lines \u003d sc.textFile(\"README.md\")\n\u003e\u003e\u003e textFile.first()\n```\n",
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:02:27.919",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cul\u003e\n  \u003cli\u003eAbrir una terminal\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eIr a la instalación Spark\u003c/p\u003e\n  \u003cpre\u003e\u003ccode class\u003d\"sh\"\u003ecd ~/spark/spark-2.2.1-bin-hadoop2.7\n\u003c/code\u003e\u003c/pre\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eArrancar el shell\u003c/p\u003e\n  \u003cpre\u003e\u003ccode class\u003d\"sh\"\u003e./bin/pyspark\n\u003c/code\u003e\u003c/pre\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eEscribir en shell (después apretar \u003ccode\u003eEnter\u003c/code\u003e)\u003c/p\u003e\n  \u003cpre\u003e\u003ccode class\u003d\"python\"\u003e\u0026gt;\u0026gt;\u0026gt; lines \u003d sc.textFile(\u0026quot;README.md\u0026quot;)\n\u0026gt;\u0026gt;\u0026gt; textFile.first()\n\u003c/code\u003e\u003c/pre\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575082947917_-1881517733",
      "id": "20171011-173126_528319238",
      "dateCreated": "2019-11-30 00:02:27.917",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Programa autónomo",
      "text": "%md\n\n* Ir a programa\n```sh\ncd diplodatos_bigdata/prog/word_count\n```\n* Actualizar repositorio\n```sh\ngit pull\n```\n* Ver programa\n```sh\nless src/main/python/WordCount.py\n```\n  (salir con [`q`])\n",
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:02:27.921",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cul\u003e\n  \u003cli\u003e\n  \u003cp\u003eIr a programa\u003c/p\u003e\n  \u003cpre\u003e\u003ccode class\u003d\"sh\"\u003ecd diplodatos_bigdata/prog/word_count\n\u003c/code\u003e\u003c/pre\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eActualizar repositorio\u003c/p\u003e\n  \u003cpre\u003e\u003ccode class\u003d\"sh\"\u003egit pull\n\u003c/code\u003e\u003c/pre\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eVer programa\u003c/p\u003e\n  \u003cpre\u003e\u003ccode class\u003d\"sh\"\u003eless src/main/python/WordCount.py\n\u003c/code\u003e\u003c/pre\u003e (salir con [\u003ccode\u003eq\u003c/code\u003e])\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575082947920_1455839618",
      "id": "20171011-175259_1199949339",
      "dateCreated": "2019-11-30 00:02:27.920",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Ejecucion de programa",
      "text": "%md\n* Ejecutar\n```sh\n~/spark/spark-2.3.4-bin-hadoop2.7/bin/spark-submit --master local[4] src/main/python/WordCount.py \\\n        ~/spark/spark-2.3.4-bin-hadoop2.7/licenses/\n```\n",
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:05:21.452",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cul\u003e\n  \u003cli\u003e\n  \u003cp\u003eEjecutar\u003c/p\u003e\n  \u003cpre\u003e\u003ccode class\u003d\"sh\"\u003e~/spark/spark-2.3.4-bin-hadoop2.7/bin/spark-submit --master local[4] src/main/python/WordCount.py \\\n    ~/spark/spark-2.3.4-bin-hadoop2.7/licenses/\n\u003c/code\u003e\u003c/pre\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575082947922_-2112411377",
      "id": "20171012-165049_905215351",
      "dateCreated": "2019-11-30 00:02:27.922",
      "dateStarted": "2019-11-30 00:05:12.848",
      "dateFinished": "2019-11-30 00:05:12.860",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Versión Spark en Zeppelin",
      "text": "%pyspark\n\nprint sc.version",
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:02:27.926",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1575082947924_-1124353061",
      "id": "20170830-114757_1684133948",
      "dateCreated": "2019-11-30 00:02:27.924",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Principales referencias online:\n\n* [Documentación Spark](http://spark.apache.org/docs/2.2.1/)\n* [API Spark Python](http://spark.apache.org/docs/2.2.1/api/python/index.html)\n",
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:02:27.928",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003ePrincipales referencias online:\u003c/h3\u003e\n\u003cul\u003e\n  \u003cli\u003e\u003ca href\u003d\"http://spark.apache.org/docs/2.2.1/\"\u003eDocumentación Spark\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href\u003d\"http://spark.apache.org/docs/2.2.1/api/python/index.html\"\u003eAPI Spark Python\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575082947926_61262202",
      "id": "20181012-171203_1400816125",
      "dateCreated": "2019-11-30 00:02:27.926",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Ejercicios MapReduce con Spark",
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:02:27.930",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eEjercicios MapReduce con Spark\u003c/h2\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575082947928_-646574877",
      "id": "20171016-172908_1510165702",
      "dateCreated": "2019-11-30 00:02:27.929",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Ejercicio 1",
      "text": "%md\n\nModifique el programa *word count* siguiente para que cuente la **cantidad de apariciones de cada letra** en el archivo.\n\n* Ayuda: solo hay que modificar la linea 6\n",
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:02:27.932",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eModifique el programa \u003cem\u003eword count\u003c/em\u003e siguiente para que cuente la \u003cstrong\u003ecantidad de apariciones de cada letra\u003c/strong\u003e en el archivo.\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eAyuda: solo hay que modificar la linea 6\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575082947931_423889579",
      "id": "20171010-202446_178633207",
      "dateCreated": "2019-11-30 00:02:27.931",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\nlines \u003d sc.textFile(\"README.md\")\n\nwords \u003d lines \\\n    .flatMap(lambda line: line.split(\" \")) \\\n    .filter(lambda word: word)\n\n\n#MapReduce\nwordCount \u003d words \\\n    .map(lambda word: (word,1)) \\\n    .reduceByKey(lambda n,m: n+m)\n\nresult \u003d wordCount \\\n    .sortBy((lambda p: p[1]), ascending \u003d False) # ordena por cantidad\n\nlocal_result \u003d result.collect() # Traigo desde cluster\n\nfor word, count in local_result[:10]: # tomo 10\n    print word, count # los imprimo\n",
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:02:27.934",
      "config": {
        "lineNumbers": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1575082947932_-1468327551",
      "id": "20171011-154135_2131718224",
      "dateCreated": "2019-11-30 00:02:27.932",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Ejercicio 2",
      "text": "%md\nCada línea del archivo `~/diplodatos_bigdata/ds/links_raw.txt` contiene un url de una página web seguido de los links que posee a otras páginas web:\n```\n\u003curl\u003e \u003curl link 1\u003e \u003curl link 2\u003e ... \u003curl link n\u003e\n```\n\nBasándose en la utilización de la técnica de *MapReduce* que se mostró en el programa `word count` haga un programa en Spark que cuente la cantidad de links que apuntan a cada página.\n\n#### Ayuda\n\nA continuación está el comienzo del programa. Falta hacer el *MapReduce* y mostrar el resultado.\n",
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:02:27.935",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eCada línea del archivo \u003ccode\u003e~/diplodatos_bigdata/ds/links_raw.txt\u003c/code\u003e contiene un url de una página web seguido de los links que posee a otras páginas web:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026lt;url\u0026gt; \u0026lt;url link 1\u0026gt; \u0026lt;url link 2\u0026gt; ... \u0026lt;url link n\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBasándose en la utilización de la técnica de \u003cem\u003eMapReduce\u003c/em\u003e que se mostró en el programa \u003ccode\u003eword count\u003c/code\u003e haga un programa en Spark que cuente la cantidad de links que apuntan a cada página.\u003c/p\u003e\n\u003ch4\u003eAyuda\u003c/h4\u003e\n\u003cp\u003eA continuación está el comienzo del programa. Falta hacer el \u003cem\u003eMapReduce\u003c/em\u003e y mostrar el resultado.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575082947934_730860411",
      "id": "20171011-175322_1451259292",
      "dateCreated": "2019-11-30 00:02:27.934",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\nbaseDir \u003d \" .... /diplodatos_bigdata\" # llenar con el directorio git\n\nlines \u003d sc.textFile(baseDir + \"/ds/links_raw.txt\")\n\nlinksTo \u003d lines \\\n    .flatMap(lambda l: l.split(\" \")[1:]) # separo los links y tomo los apuntados\n\n# Ahora linksTo tiene las paginas apuntadas\n\n# Completar los ...\n\n# MapReduce\ninvLinkCount \u003d linksTo.map(...) \\\n    .reduceByKey(...)\n\nresult \u003d invLinkCount.sortBy((lambda p: ...), ascending \u003d False)\n\nlocal_result \u003d result.collect() # Traigo desde cluster\n\nfor word, count in local_result[:10]: # tomo 10\n    print word, count # los imprimo\n",
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:02:27.937",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1575082947936_-1654070218",
      "id": "20191121-184701_1405603118",
      "dateCreated": "2019-11-30 00:02:27.936",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "FIN",
      "text": "//val baseDir\u003d\"https://git.cs.famaf.unc.edu.ar/dbarsotti/diplodatos_bigdata/raw/master/clases\"\nval baseDir\u003d\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases\"\n\nz.put(\"baseDir\", baseDir)\nprint(\"\"\"%html\n\u003cscript\u003e\n    var heads \u003d document.getElementsByTagName(\u0027h2\u0027);\n    var numHeads \u003d heads.length;\n    var inner \u003d \"\";\n    var i \u003d 0;\n    var j \u003d 0;\n    while (i \u003c numHeads){\n        inner \u003d heads[i].innerHTML;\n        if (inner.search(\".-\") !\u003d -1 ) {\n            j++;\n            heads[i].innerHTML \u003d inner.replace(/(~|\\d+)\\.-/,\"\"+j+\".-\");\n        }\n        i++\n    }\n\u003c/script\u003e\n\"\"\")",
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:02:27.938",
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "fontSize": 9.0,
        "title": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cscript\u003e\n    var heads \u003d document.getElementsByTagName(\u0027h2\u0027);\n    var numHeads \u003d heads.length;\n    var inner \u003d \"\";\n    var i \u003d 0;\n    var j \u003d 0;\n    while (i \u003c numHeads){\n        inner \u003d heads[i].innerHTML;\n        if (inner.search(\".-\") !\u003d -1 ) {\n            j++;\n            heads[i].innerHTML \u003d inner.replace(/(~|\\d+)\\.-/,\"\"+j+\".-\");\n        }\n        i++\n    }\n\u003c/script\u003e\nbaseDir: String \u003d https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1575082947937_1465056910",
      "id": "20160712-175904_2058049512",
      "dateCreated": "2019-11-30 00:02:27.937",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "user": "anonymous",
      "dateUpdated": "2019-11-30 00:02:27.939",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1575082947938_1570073889",
      "id": "20191129-104030_1246831484",
      "dateCreated": "2019-11-30 00:02:27.938",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Diplodatos/Clase 01 - Introducción a Spark",
  "id": "2EVQU5H9P",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}