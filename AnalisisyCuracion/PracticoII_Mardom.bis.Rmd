---
title: "R Notebook"
output:
  html_notebook:
    fig_width: 6 
    fig_height: 4
    fig_align: 'center'
---

```{r}
library(RColorBrewer)

knitr::opts_chunk$set(fig.width=12, fig.height=8) 
```

Carga de datos

```{r}
data = read.csv("datos/wine.data",header=F)
attach(data)
```

Nombramos los campos del DataSet ya que no se encuentran en el archivo de entrada

```{r}
namelist <- c('class','Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols','Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue','OD280/OD315 of diluted wines','Proline')

colnames(data) <- namelist
```

Vemos la dimensión de los datos

```{r}
dim(data)
```

Exploración de la estructura del DataSet

```{r}
str(data)
```

Definimos la función para normalizar los datos, en este caso utilizamos MinMax.

```{r}
normalize <- function(x) {
  return ((x-min(x))/(max(x)-min(x)))
}
```

Aplicamos la normalización a los datos, ignoramos la columna 1 ya que la misma corresponde a las clases que queremos identificar. Luego, añadimos la columna "class" para el análisis siguiente.

```{r}
data_normed <- as.data.frame(lapply(data[c(2:ncol(data))], normalize))
data_normed$class <- data$class
str(data_normed)
```

Graficamos Boxplots

```{r fig.align='center',fig.height = 6, fig.width = 6}

m = as.matrix(data_normed[1:ncol(data_normed)-1])

cols  <- colorRampPalette(brewer.pal(12, "Set3"), alpha=TRUE)(ncol(m))
colsm <-matrix(rep(cols, each=nrow(m)), ncol=ncol(m))

boxplot(m~col(m), horizontal=TRUE, outline=TRUE, lty=1, staplewex=0, boxwex=0.8, boxlwd=1, medlwd=1, col=cols, xaxt="n", yaxt="n")


axis(side=1, at=seq(0,1,by=0.1), cex.axis=0.8, lty=0, tick=NA, line=-1)
axis(side=1, at=0.5, labels="Distribución normalizada", lty=0, tick=NA)
axis(side=2, at=1:13, cex.axis=0.8, lty=0, tick=NA, labels=colnames(m), las=2)
axis(side=2, at=7/2, labels="Variable", lty=0, tick=NA, las=3, line=6)
```

Definimos una función para hacer la remoción de los outliers

```{r}
removeOutliers <- function(data,n=3){
  f <- !logical(length = nrow(data))
  for (c in colnames(data)) {
    x <- data[[c]]
    g <- x > quantile(x,.25) - 0.5*n*IQR(x) & x < quantile(x,.75) + 0.5*n*IQR(x)
    f <- f & g
  }
  return(f)
}
```

Removemos outliers que se encuentran más allá de +/- 1.5*IQR(x) del primer y tercer cuartil.

```{r}
filters <- removeOutliers(data_normed[1:ncol(data_normed)-1])
data_cleaned <- subset(data_normed,filters)
```

Re-dibujamos los boxplots y observamos que los outiliers más marcados se fueron

```{r  fig.align='center',fig.height = 6, fig.width = 6}
m = as.matrix(data_cleaned[1:ncol(data_cleaned)-1])

cols  <- colorRampPalette(brewer.pal(12, "Set3"), alpha=TRUE)(ncol(m))
colsm <-matrix(rep(cols, each=nrow(m)), ncol=ncol(m))

boxplot(m~col(m), horizontal=TRUE, outline=TRUE, lty=1, staplewex=0, boxwex=0.8, boxlwd=1, medlwd=1, col=cols, xaxt="n", yaxt="n")

# add axes and title
axis(side=1, at=seq(0,1,by=0.1), cex.axis=0.8, lty=0, tick=NA, line=-1)
axis(side=1, at=0.5, labels="Distribución normalizada", lty=0, tick=NA)
axis(side=2, at=1:13, cex.axis=0.8, lty=0, tick=NA, labels=colnames(m), las=2)
axis(side=2, at=7/2, labels="Variable", lty=0, tick=NA, las=3, line=6)
```

Plot del DataSet coloreado por las clases



```{r  fig.align='center',fig.height = 10, fig.width = 10}
plot(data_cleaned[, 1:ncol(data_cleaned)-1], col = data_cleaned$class, pch = '*', main = "Wine Dataset")
```

Aplicamos clustering jeráctico del paquete mclust, con nclusters = 3

```{r}
library(mclust)

initialk <- mclust::hc(data = data_cleaned, modelName = "EII")
initialk <- mclust::hclass(initialk,3)
```

```{r}
mcl.model <- Mclust(data_cleaned[, 1:ncol(data_cleaned)-1], 3,verbose=TRUE)
```

```{r fig.align='center',fig.height = 10, fig.width = 10}
plot(mcl.model, what = "classification", main = "Mclust Classification")
```

Posición de los centroides

```{r}
mcl.model$parameters$mean
```

Tabla de comparación

```{r}
table(mcl.model$classification, data_cleaned$class)
```

Computamos el error del clustering

```{r}
error <- classError(mcl.model$classification,data_cleaned$class)
print(error)
```

Calculamos el clustering variando el número de clusters y graficamos el error vs el número de clusters
```{r}

Ks_mclust <- vector()
cError_mclust <- vector()

for(k in 1:6){
  mcl.model <- Mclust(data_cleaned[, 1:ncol(data_cleaned)-1], k,verbose=FALSE)
  Ks_mclust <- append(Ks_mclust,k)
  cError_mclust <- append(cError_mclust,classError(mcl.model$classification,data_cleaned$class)$errorRate)
}

plot(Ks_mclust,cError_mclust,xlab="Número de grupos",ylab="Error")
```

Aplicamos el algoritmo de Kmeans con k = 3

```{r echo=TRUE}
set.seed(20)
DataCluster <- kmeans(data_cleaned[, 1:ncol(data_cleaned)-1], 3, nstart = 20)
DataCluster
```

Tabla de comparación
```{r}
table(DataCluster$cluster, data_cleaned$class)
```

Error

```{r}
error <- classError(DataCluster$cluster,data_cleaned$class)
print(error)
```

Aplicamos Kmeans variando el número de K

```{r echo=TRUE}

Ks_kmeans <- vector()
cError_kmeans <- vector()

for(k in 1:6){
  DataCluster <- kmeans(data_cleaned[, 1:ncol(data_cleaned)-1], k, nstart = 20)
  Ks_kmeans <- append(Ks_kmeans,k)
  cError_kmeans <- append(cError_kmeans,classError(DataCluster$cluster,data_cleaned$class)$errorRate)
}

plot(Ks_mclust,cError_mclust,col = 'forestgreen',xlab = "Número de grupos",ylab = "Error")
points(Ks_kmeans,cError_kmeans,col = 'blue')
legend("topright", inset=.05, title="Algoritmo", c("Mclust","Kmeans"), fill=c('forestgreen','blue'), horiz=TRUE)

```



Empleando esta métrica para el error se observa que el mismo disminuye a medida que aumentamos el número de clusters que se quieren identificar hasta llegar al valor de clusters igual a 3 (que se corresponde con la cantidad que está en la DataSet). Luego, para 4 y 5 cluster el error vuelve a aumentar.

Ahora queremos ver que sucede con los datos limpiados por outliers pero sin la normalización.

```{r}
filters = removeOutliers(data[2:ncol(data)])
data_raw_cleaned <- subset(data,filters)
```

```{r echo=TRUE}
set.seed(10)
DataCluster <- kmeans(data_raw_cleaned[, 2:ncol(data_raw_cleaned)], 3, nstart = 20)
```

```{r}
table(DataCluster$cluster, data_raw_cleaned$class)
```


```{r}
classError(DataCluster$cluster,data_raw_cleaned$class)
```

Observamos que los valores mal clasificados aumentaron, como así también el error en casi un orden de magnitud respecto a la clasificación utilizando los datos normalizados.

Comparación normalizado vs no-normalizado

```{r echo=TRUE}

Ks_unnormed <- vector()
cError_unnormed <- vector()

for(k in 2:6){
  DataCluster <- kmeans(data_raw_cleaned[, 2:ncol(data_raw_cleaned)], k, nstart = 20)
  Ks_unnormed <- append(Ks_unnormed,k)
  cError_unnormed <- append(cError_unnormed,classError(DataCluster$cluster,data_raw_cleaned$class)$errorRate)
}

plot(Ks_kmeans,cError_kmeans,col = 'forestgreen',xlab = "Número de grupos",ylab = "Error")
points(Ks_unnormed,cError_unnormed,col = 'blue')
legend("topright", inset=.05, title="Muestra", c("Normalizada","Sin Normalizar"), fill=c('forestgreen','blue'), horiz=TRUE)

```
En la mayoría de los casos utilizando los datos sin normalizar aumenta el error.


Determinando el número óptimo de grupos

Para ello utilizaremos el coeficiente Silhouette (Si) que mide que tan agrupada está una observación y estima la distancia entre los grupos.
  - Observaciones con Si grande (cercano a 1) están muy bien agrupadas
  - Un valor de Si bajo (alrededor de 0) indica que la observación se encuentra entre dos grupos
  - Observacions con un valor de Si negativo están probablemente localizadas en el grupo incorrecto.

Como queremos determinar el coeficiente Silhouette variando el hiperparámetro K (número de grupos) lo que graficaremos será el valor medio de Si para todas las observaciones en función de K

```{r}
library(cluster)

silhouette_score <- function(k){
  km <- kmeans(data_cleaned, centers = k, nstart=25)
  ss <- silhouette(km$cluster, dist(data_cleaned))
  mean(ss[, 3])
}

k <- 2:10
avg_sil <- sapply(k, silhouette_score)
plot(k, type='b', avg_sil, xlab='Número de grupos', ylab='Promedio Silhouette', frame=FALSE)
```

Observamos que el mejor (más grande) coeficiente Si se obtiene para el caso en que K = 3, precisamente el número de grupos que conociamos desde la base de datos.


