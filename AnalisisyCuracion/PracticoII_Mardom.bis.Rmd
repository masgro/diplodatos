---
title: "R Notebook"
output:
  html_notebook:
    fig_width: 6 
    fig_height: 4
    fig_align: 'center'
---

```{r}
library(RColorBrewer)

knitr::opts_chunk$set(fig.width=12, fig.height=8) 
```

Carga de datos

```{r}
data = read.csv("datos/wine.data",header=F)
attach(data)
```

Nombramos los campos del DataSet ya que no se encuentran en el archivo de entrada

```{r}
namelist <- c('class','Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols','Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue','OD280/OD315 of diluted wines','Proline')

colnames(data) <- namelist
```

Vemos la dimensión de los datos

```{r}
dim(data)
```

Exploración de la estructura del DataSet

```{r}
str(data)
```

Definimos la función para normalizar los datos, en este caso utilizamos MinMax.

```{r}
normalize <- function(x) {
  return ((x-min(x))/(max(x)-min(x)))
}
```

Aplicamos la normalización a los datos, ignoramos la columna 1 ya que la misma corresponde a las clases que queremos identificar. Luego, añadimos la columna "class" para el análisis siguiente.

```{r}
data_normed <- as.data.frame(lapply(data[c(2:ncol(data))], normalize))
data_normed$class <- data$class
str(data_normed)
```

Graficamos Boxplots

```{r fig.align='center',fig.height = 6, fig.width = 6}

m = as.matrix(data_normed[1:ncol(data_normed)-1])

cols  <- colorRampPalette(brewer.pal(12, "Set3"), alpha=TRUE)(ncol(m))
colsm <-matrix(rep(cols, each=nrow(m)), ncol=ncol(m))

boxplot(m~col(m), horizontal=TRUE, outline=TRUE, lty=1, staplewex=0, boxwex=0.8, boxlwd=1, medlwd=1, col=cols, xaxt="n", yaxt="n")


axis(side=1, at=seq(0,1,by=0.1), cex.axis=0.8, lty=0, tick=NA, line=-1)
axis(side=1, at=0.5, labels="Distribución normalizada", lty=0, tick=NA)
axis(side=2, at=1:13, cex.axis=0.8, lty=0, tick=NA, labels=colnames(m), las=2)
axis(side=2, at=7/2, labels="Variable", lty=0, tick=NA, las=3, line=6)
```

Definimos una función para hacer la remoción de los outliers

```{r}
removeOutliers <- function(data,n=3){
  f <- !logical(length = nrow(data))
  for (c in colnames(data)) {
    x <- data[[c]]
    g <- x > quantile(x,.25) - 0.5*n*IQR(x) & x < quantile(x,.75) + 0.5*n*IQR(x)
    f <- f & g
  }
  return(f)
}
```

Removemos outliers que se encuentran más allá de +/- 1.5*IQR(x) del primer y tercer cuartil.

```{r}
filters <- removeOutliers(data_normed[1:ncol(data_normed)-1])
data_cleaned <- subset(data_normed,filters)
```

Re-dibujamos los boxplots y observamos que los outiliers más marcados se fueron

```{r  fig.align='center',fig.height = 6, fig.width = 6}
m = as.matrix(data_cleaned[1:ncol(data_cleaned)-1])

cols  <- colorRampPalette(brewer.pal(12, "Set3"), alpha=TRUE)(ncol(m))
colsm <-matrix(rep(cols, each=nrow(m)), ncol=ncol(m))

boxplot(m~col(m), horizontal=TRUE, outline=TRUE, lty=1, staplewex=0, boxwex=0.8, boxlwd=1, medlwd=1, col=cols, xaxt="n", yaxt="n")

# add axes and title
axis(side=1, at=seq(0,1,by=0.1), cex.axis=0.8, lty=0, tick=NA, line=-1)
axis(side=1, at=0.5, labels="Assigned Probability %", lty=0, tick=NA)
axis(side=2, at=1:13, cex.axis=0.8, lty=0, tick=NA, labels=colnames(m), las=2)
axis(side=2, at=7/2, labels="Phrase", lty=0, tick=NA, las=3, line=6)
title("Perceptions of Probability")
```

Plot del DataSet coloreado por las clases



```{r  fig.align='center',fig.height = 10, fig.width = 10}
plot(data_cleaned[, 1:ncol(data_cleaned)-1], col = data_cleaned$class, pch = '*', main = "Wine Dataset")
```

Aplicamos clustering jeráctico del paquete mclust, con nclusters = 3

```{r}
library(mclust)

initialk <- mclust::hc(data = data_cleaned, modelName = "EII")
initialk <- mclust::hclass(initialk,3)
```

```{r}
mcl.model <- Mclust(data_cleaned[, 1:ncol(data_cleaned)-1], 3,verbose=TRUE)
```

```{r fig.align='center',fig.height = 10, fig.width = 10}
plot(mcl.model, what = "classification", main = "Mclust Classification")
```

Posición de los centroides

```{r}
mcl.model$parameters$mean
```

Tabla de comparación

```{r}
table(mcl.model$classification, data_cleaned$class)
```

Computamos el error del clustering

```{r}
classError(mcl.model$classification,data_cleaned$class)
```

Calculamos el clustering variando el número de clusters y graficamos el error vs el número de clusters
```{r}

Ks <- vector()
cError <- vector()

for(k in 1:5){
  mcl.model <- Mclust(data_cleaned[, 1:ncol(data_cleaned)-1], k,verbose=FALSE)
  Ks <- append(Ks,k)
  cError <- append(cError,classError(mcl.model$classification,data_cleaned$class)$errorRate)
}

print(Ks)
print(cError)
plot(Ks,cError)
```

Aplicamos el algoritmo de Kmeans con k = 3

```{r echo=TRUE}
set.seed(20)
DataCluster <- kmeans(data_cleaned[, 1:ncol(data_cleaned)-1], 3, nstart = 20)
DataCluster
```

Tabla de comparación


```{r}
table(DataCluster$cluster, data_cleaned$class)
```

Error

```{r}
classError(DataCluster$cluster,data_cleaned$class)
```

Aplicamos Kmeans variando el número de K

```{r echo=TRUE}

Ks <- vector()
cError <- vector()

for(k in 1:5){
  DataCluster <- kmeans(data_cleaned[, 1:ncol(data_cleaned)-1], k, nstart = 20)
  Ks <- append(Ks,k)
  cError <- append(cError,classError(DataCluster$cluster,data_cleaned$class)$errorRate)
}

print(Ks)
print(cError)
plot(Ks,cError)
```

Empleando esta métrica para el error se observa que el mismo disminuye a medida que aumentamos el número de clusters que se quieren identificar hasta llegar al valor de clusters igual a 3 (que se corresponde con la cantidad que está en la DataSet). Luego, para 4 y 5 cluster el error vuelve a aumentar.

Ahora queremos ver que sucede con los datos limpiados por outliers pero sin la normalización.

```{r}
filters = removeOutliers(data[2:ncol(data)])
data_raw_cleaned <- subset(data,filters)
```

```{r echo=TRUE}
set.seed(10)
DataCluster <- kmeans(data_raw_cleaned[, 2:ncol(data_raw_cleaned)], 3, nstart = 20)
```

```{r}
table(DataCluster$cluster, data_raw_cleaned$class)
```

```{r}
classError(DataCluster$cluster,data_raw_cleaned$class)
```

Comparación normalizado vs no-normalizado

```{r echo=TRUE}

Ks_unnormed <- vector()
cError_unnormed <- vector()

for(k in 1:5){
  DataCluster <- kmeans(data_raw_cleaned[, 2:ncol(data_raw_cleaned)], k, nstart = 20)
  Ks_unnormed <- append(Ks_unnormed,k)
  cError_unnormed <- append(cError_unnormed,classError(DataCluster$cluster,data_raw_cleaned$class)$errorRate)
}

print(Ks)
print(Ks_unnormed)

print(cError)
print(cError_unnormed)

plot(c(Ks,Ks_unnormed),c(cError,cError_unnormed),col = rep(c('forestgreen','blue'),each = 6))
```

Determinando el número óptimo de grupos

Para ello utilizaremos el coeficiente Silhouette (Si) que mide que tan agrupada está una observación y estima la distancia entre los grupos.
  - Observaciones con Si grande (cercano a 1) están muy bien agrupadas
  - Un valor de Si bajo (alrededor de 0) indica que la observación se encuentra entre dos grupos
  - Observacions con un valor de Si negativo están probablemente localizadas en el grupo incorrecto.

Como queremos determinar el coeficiente Silhouette variando el hiperparámetro K (número de grupos) lo que graficaremos será el valor medio de Si para todas las observaciones en función de K

```{r}
silhouette_score <- function(k){
  km <- kmeans(data_cleaned, centers = k, nstart=25)
  ss <- silhouette(km$cluster, dist(data_cleaned))
  mean(ss[, 3])
}

k <- 2:10
avg_sil <- sapply(k, silhouette_score)
plot(k, type='b', avg_sil, xlab='Número de grupos', ylab='Promedio Silhouette', frame=FALSE)
```

Observamos que el mejor (más grande) coeficiente Si se obtiene para el caso en que K = 3, precisamente el número de grupos que conociamos desde la base de datos.


